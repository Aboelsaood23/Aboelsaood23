{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtuKbMKucM1YMCrYOP/Xlw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aboelsaood23/Aboelsaood23/blob/main/Deep_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**NumPy Library :**\n",
        "   it is a Python library for numerical computing that provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.\n",
        "\n",
        "  NumPy's importance in neural networks lies in its fundamental role in handling data efficiently. Here's why NumPy is crucial for neural networks:\n",
        "*   Efficient numerical computations\n",
        "*   Array operations\n",
        "*   Integration with deep learning frameworks\n",
        "*   Compatibility with scientific computing libraries"
      ],
      "metadata": {
        "id": "r68fVK1-3r82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Sigmoid Function:**\n",
        "*   The sigmoid function, often denoted as Ïƒ(x), is a mathematical function that maps any real-valued number to a value between 0 and 1. It has an S-shaped curve and is widely used in various fields\n",
        "*   working with NumPy arrays, np.exp() is generally preferable to math.exp(), for several reasons :\n",
        "   1.   Broadcasting\n",
        "   2.   Performance\n",
        "   3.   *Consistency*"
      ],
      "metadata": {
        "id": "cxbzA8D46AXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "def basic_sigmoid(x):\n",
        "  s =1/(1+math.exp(-x))\n",
        "  return s\n",
        "#check performance of the basic function\n",
        "print(\"basic sigmoid function using math.exp = \",basic_sigmoid(3) )\n",
        "\n",
        "#we rarely use the \"math\" library in deep learning because the inputs of the functions are real numbers.\n",
        "#In deep learning we mostly use matrices and vectors\n",
        "\n",
        "x =np.array([1,2,3]) # this array can't be passed to basic sigmoid function that uses math.exp()\n",
        "def sigmoid(x):\n",
        "  s =1/(1+np.exp(-x))\n",
        "  return s\n",
        "#check performance of this function\n",
        "print(\"sigmoid of new x :\",sigmoid(x))\n",
        "\n"
      ],
      "metadata": {
        "id": "K7gnE_kS9t6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "986d1bc3-98e9-4f29-af78-9e449bbfbca4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic sigmoid function using math.exp =  0.9525741268224334\n",
            "sigmoid of new x : [0.73105858 0.88079708 0.95257413]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Sigmoid derivative :**\n",
        "*   will be used on gradient functions and to optimize loss function = sigmoid(x)(1-sigmoid(x))\n",
        "\n"
      ],
      "metadata": {
        "id": "0PQrPkie-0c9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def derivative_sigmoid(x):\n",
        "  s = sigmoid(x) #calling sigmoid function\n",
        "  ds= s*(1-s)\n",
        "  return s\n",
        "\n",
        "print(\"sigmoid derivative df x = \",derivative_sigmoid(x))"
      ],
      "metadata": {
        "id": "WEIlHgU29t1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cff6f58b-099a-4e0c-d12e-224e957b8d9c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sigmoid derivative df x =  [0.73105858 0.88079708 0.95257413]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Reshaping arrays using numpy :**\n",
        "*   Two common numpy functions used in deep learning are np.shape and np.reshape().\n",
        "* X.shape is used to get the shape (dimension) of a matrix/vector X.\n",
        "* X.reshape(...) is used to reshape X into some other dimension.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "opKVso1n_bkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement image2vector() that takes an input of shape (length, height, 3) and returns a vector of shape (length*height*3, 1).\n",
        "def image2vector(image):\n",
        "  v =image.reshape(image.shape[0]*image.shape[1]*image.shape[2],1)\n",
        "  return v\n",
        "\n",
        "#check performance of this function\n",
        "image = np.array([[[ 0.67826139,  0.29380381],\n",
        "        [ 0.90714982,  0.52835647],\n",
        "        [ 0.4215251 ,  0.45017551]],\n",
        "\n",
        "       [[ 0.92814219,  0.96677647],\n",
        "        [ 0.85304703,  0.52351845],\n",
        "        [ 0.19981397,  0.27417313]],\n",
        "\n",
        "       [[ 0.60659855,  0.00533165],\n",
        "        [ 0.10820313,  0.49978937],\n",
        "        [ 0.34144279,  0.94630077]]])\n",
        "print(\"\\nshape of original image\",image.shape)\n",
        "print(\"shape of reshaped image\",image2vector(image).shape)\n",
        "print(\"new reshaped image\",image2vector(image))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-BGzjj0_ipv",
        "outputId": "4d17d77c-39de-4ae1-e4db-dddf41f5e311"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "shape of original image (3, 3, 2)\n",
            "shape of reshaped image (18, 1)\n",
            "new reshaped image [[0.67826139]\n",
            " [0.29380381]\n",
            " [0.90714982]\n",
            " [0.52835647]\n",
            " [0.4215251 ]\n",
            " [0.45017551]\n",
            " [0.92814219]\n",
            " [0.96677647]\n",
            " [0.85304703]\n",
            " [0.52351845]\n",
            " [0.19981397]\n",
            " [0.27417313]\n",
            " [0.60659855]\n",
            " [0.00533165]\n",
            " [0.10820313]\n",
            " [0.49978937]\n",
            " [0.34144279]\n",
            " [0.94630077]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Normalizing rows using numpy :**\n",
        "*   Another common technique we use in Machine Learning and Deep Learning is to normalize our data.\n",
        "* It often leads to a better performance because gradient descent converges faster after normalization.\n",
        "* by normalization we mean changing x to x/||x|| (dividing each row vector of x by its norm).\n",
        "* x_norm takes the norm of each row of x. So x_norm has the same number of rows but only 1 column,So how did it work when you divided x by x_norm? This is called broadcasting"
      ],
      "metadata": {
        "id": "-R1xBkeeAXky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizeRows(x):\n",
        "  x_norm =np.linalg.norm(x,axis=1,keepdims =True)\n",
        "  #axis=1 means that the norm is calculated along the second axis,So, the norm is calculated for each row of the array.\n",
        "  #keepdims = True : This parameter, when set to True, ensures that the dimensions of the output are the same as the input\n",
        "  x =x/x_norm\n",
        "  return x\n",
        "\n",
        "#check performance of this function\n",
        "x =np.array([\n",
        "    [0,3,4],\n",
        "    [1,6,4]\n",
        "])\n",
        "print(\"normalizeRows(x)= \"+str(normalizeRows(x)))\n",
        "print(\"shape before noermalization\",x.shape)\n",
        "print(\"shape after normalization\",normalizeRows(x).shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w583BuVpAS9o",
        "outputId": "7dc328ea-23b3-4634-fdc2-14aca19375c4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "normalizeRows(x)= [[0.         0.6        0.8       ]\n",
            " [0.13736056 0.82416338 0.54944226]]\n",
            "shape before noermalization (2, 3)\n",
            "shape after normalization (2, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**softmax using numpy :**\n",
        "*   it a popular activation function used in neural networks, particularly in the output layer for multi-class classification tasks\n",
        "*  It takes a vector of real numbers as input and outputs a probability distribution over multiple classes, ensuring that the sum of the probabilities adds up to 1."
      ],
      "metadata": {
        "id": "azDUmASGCFEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  x_exp =np.exp(x)\n",
        "  x_sum =np.sum(x_exp,axis =1,keepdims=True)\n",
        "  s =x_exp/x_sum\n",
        "  return s\n",
        "\n",
        "#check performance of this function\n",
        "x =np.array([[9,2,5,0,0],\n",
        "             [7,5,0,0,0]])\n",
        "print(\"softmax(x) = \"+str(softmax(x)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5FGFddwDBzj",
        "outputId": "d884c227-bc75-456f-ff7a-8ee5d4b18e6d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
            "  1.21052389e-04]\n",
            " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
            "  8.01252314e-04]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Vectorization usnig NumPy :**\n",
        "*   technique used in computer programming and numerical computing to perform operations on entire arrays or vectors of data at once, rather than on individual elements sequentially\n",
        "* we will implement three operations to check differnce on performance using victorizations and normal implementation :\n",
        "   1. DOT PRODUCT :  multiply each elemnent by corresponding element, then do summation for all results\n",
        "   2. OUTER PRODUCT : multiply each element on x1 to all elements of x2, then move to next element and so on\n",
        "   3. ELEMENTWISE : multiply each element on crossponding element of x2"
      ],
      "metadata": {
        "id": "Fefoej-2EgTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
        "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
        "\n",
        "#classic dot product\n",
        "tic =time.process_time()\n",
        "dot =0\n",
        "for i in range(len(x1)):\n",
        "  dot = x1[i]*x2[i]\n",
        "toc =time.process_time()\n",
        "print(\"dot = \"+str(dot)+\"\\n computation time for classic dot product =\"+str(1000*(toc-tic))+\"ms\")\n",
        "\n",
        "\n",
        "#classic outer product\n",
        "tic =time.process_time()\n",
        "outer =np.zeros((len(x1),len(x2)))\n",
        "for i in range(len(x1)):\n",
        "  for j in range(len(x2)):\n",
        "    outer[i,j] =x1[i]*x2[i]\n",
        "\n",
        "toc =time.process_time()\n",
        "print(\"\\n\\nouter : \"+str(outer)+\"\\ncomputation time for classic outer product\"+str(1000*(toc-tic))+\"ms\")\n",
        "\n",
        "\n",
        "#classi element wise\n",
        "tic =time.process_time()\n",
        "mul =np.zeros(len(x1))\n",
        "for i in range(len(x1)):\n",
        "  mul[i] =x1[i]*x2[i]\n",
        "toc =time.process_time()\n",
        "print(\"\\n\\nelementwie multiplication = \"+str(mul)+\"\\n computation time for classic elemnetwise \"+str(1000*(toc-tic))+\"ms\")\n",
        "\n",
        "\n",
        "#Do all sam operations using vectoriation\n",
        "print(\"\\n\\nVectorized Resluts\")\n",
        "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
        "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
        "\n",
        "#dot product\n",
        "tic = time.process_time()\n",
        "dot = np.dot(x1,x2)\n",
        "toc = time.process_time()\n",
        "print (\"\\n\\ndot = \" + str(dot) + \"\\nComputation time for dot product vectorized = \" + str(1000*(toc - tic)) + \"ms\")\n",
        "\n",
        "#Outer product\n",
        "tic  =time.process_time()\n",
        "outer=np.outer(x1,x2)\n",
        "toc  =time.process_time()\n",
        "print(\"\\n\\nelementwise multiplication =\"+str(mul)+\"\\n computation time for outer product vectorized =\"+str(1000*(toc-tic))+\"ms\")\n",
        "\n",
        "#elementwise\n",
        "tic = time.process_time()\n",
        "mul = np.multiply(x1,x2)\n",
        "toc = time.process_time()\n",
        "print (\"\\n\\nelementwise multiplication = \" + str(mul) + \"\\ncomputation time for elementwise vectorized = \" + str(1000*(toc - tic)) + \"ms\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLsgRuVfETEU",
        "outputId": "9df7f267-473a-4523-8c01-e9d0bf491a29"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dot = 0\n",
            " computation time for classic dot product =0.18673400000324136ms\n",
            "\n",
            "\n",
            "outer : [[81. 81. 81. 81. 81. 81. 81. 81. 81. 81. 81. 81. 81. 81. 81.]\n",
            " [ 4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.]\n",
            " [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [63. 63. 63. 63. 63. 63. 63. 63. 63. 63. 63. 63. 63. 63. 63.]\n",
            " [10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [81. 81. 81. 81. 81. 81. 81. 81. 81. 81. 81. 81. 81. 81. 81.]\n",
            " [ 4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.  4.]\n",
            " [25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
            "computation time for classic outer product0.42043300000216277ms\n",
            "\n",
            "\n",
            "elementwie multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]\n",
            " computation time for classic elemnetwise 0.22722699999633278ms\n",
            "\n",
            "\n",
            "Vectorized Resluts\n",
            "\n",
            "\n",
            "dot = 278\n",
            "Computation time for dot product vectorized = 0.18234100000569242ms\n",
            "\n",
            "\n",
            "elementwise multiplication =[81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]\n",
            " computation time for outer product vectorized =0.16077500000477585ms\n",
            "\n",
            "\n",
            "elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]\n",
            "computation time for elementwise vectorized = 0.13823199999762892ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E3Gg4Lo3DA9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9gODqymndp6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ef3OhgAAD8h5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}